\section{ Learning Theory\hpoints{25}}

\begin{enumerate}[1.]
\item
In a supervised learning problem, the true quantity we really want to minimize is
\begin{enumerate}[(a)]
\item the training error
\item the test error
\item the generalization error
\item the cross-validation error
\item none of the above
\end{enumerate}
%
\vspace{4pt}
%
\item
Which of the following serve to provide an estimate of the generalization error? Pick all that apply.
\begin{enumerate}[(a)]
\item training error
\item test error
\item cross-validation error
\item VC-complexity bound
\item none of the above
\end{enumerate}
%
\vspace{4pt}
%
\item
Which of the following tasks can VC-complexity bounds be useful for? Pick all that apply.
\begin{enumerate}[(a)]
\item obtaining high-confidence bounds on generalization error
\item choosing a training set
\item choosing folds for cross-validation
\item model selection
\item none of the above
\end{enumerate}
%
\vspace{4pt}
%
\item
For binary classification with 0-1 loss, an algorithm that always chooses an optimal linear classifier on the training data (with smallest 0-1 error on the training data) is
\begin{enumerate}[(a)]
\item never consistent
\item consistent for some data distributions
\item universally consistent
\item an optimal algorithm for 0-1 loss
\item none of the above
\end{enumerate}
%
\vspace{4pt}
%
\item
For binary classification with 0-1 loss, which of the following algorithms are universally consistent? Pick all that apply.
\begin{enumerate}[(a)]
\item SVM with quadratic kernel and suitably chosen $C$ parameter
\item SVM with RBF kernel and suitably chosen $C$ parameter
\item Linear logistic regression with $L_2$ regularization and suitably chosen $\lambda$ parameter 
\item Logistic regression with RBF kernel, RKHS regularization, and suitably chosen $\lambda$ parameter 
\item none of the above
\end{enumerate}
\end{enumerate}
