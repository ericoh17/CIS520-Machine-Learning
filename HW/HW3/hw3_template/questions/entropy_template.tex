\section{Entropy and Minimum Description Length}


\begin{enumerate}

\item  You will need to transmit a sequence of $n$ binary observations (e.g. y values),
which will be ``1" with probability $p_1 = 3/16$ and ``0" with probability $p_0 =13/16$.
What is the minimum number of bits to code the sequence (for large n)? Please do calculate the number instead of only providing the equation.


\item  You are doing feature selection where there are far more possible features than observations. Assume there are total of $f$ features and
 roughly $3/16$ of the features will be selected. The original penalty parameter $\lambda$ in RIC($Err/2\sigma^2 + \lambda ||w||_0$) is $\log_2 f$. In this situation, what would be a better alternative to $\lambda$?


\end{enumerate}


